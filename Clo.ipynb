{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08d16a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully with shape: (18787, 2)\n",
      "Missing values: 0\n",
      "Duplicate rows: 194\n",
      "Class distribution:\n",
      "new_label\n",
      "Normal        6751\n",
      "Depression    4369\n",
      "Anxiety       4175\n",
      "Stress        2698\n",
      "Suicide        600\n",
      "Name: count, dtype: int64\n",
      "==================================================\n",
      "DATASET ANALYSIS\n",
      "==================================================\n",
      "Average text length: 283.33\n",
      "Average word count: 55.71\n",
      "Max text length: 19822\n",
      "Min text length: 3\n",
      "\n",
      "Class distribution:\n",
      "Normal: 6751 (36.31%)\n",
      "Depression: 4369 (23.50%)\n",
      "Anxiety: 4175 (22.45%)\n",
      "Stress: 2698 (14.51%)\n",
      "Suicide: 600 (3.23%)\n",
      "Preprocessing text data...\n",
      "Vocabulary size: 54908\n",
      "Average sequence length: 36.05\n",
      "Number of classes: 5\n",
      "Classes: ['Anxiety' 'Depression' 'Normal' 'Stress' 'Suicide']\n",
      "Training set: (11750, 150)\n",
      "Validation set: (2938, 150)\n",
      "Test set: (3673, 150)\n",
      "Class weights: {0: np.float64(0.8831266441187523), 1: np.float64(0.8639705882352942), 2: np.float64(0.5504802061372687), 3: np.float64(1.3694638694638694), 4: np.float64(6.119791666666667)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_3       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_3         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,000,000</span> │ input_layer_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ spatial_dropout1d_3 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout1D</span>)  │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_5         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_6     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">336,896</span> │ spatial_dropout1… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │ not_equal_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ bidirectional_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │ not_equal_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_7     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">164,352</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │ not_equal_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ bidirectional_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │ not_equal_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │ not_equal_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_3       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_max_pooli… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │ concatenate_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ dense_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ dropout_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ dense_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">645</span> │ dropout_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_3       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_3         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m200\u001b[0m)  │  \u001b[38;5;34m3,000,000\u001b[0m │ input_layer_3[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ spatial_dropout1d_3 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m200\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ embedding_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mSpatialDropout1D\u001b[0m)  │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_5         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ input_layer_3[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_6     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │    \u001b[38;5;34m336,896\u001b[0m │ spatial_dropout1… │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │ not_equal_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │      \u001b[38;5;34m1,024\u001b[0m │ bidirectional_6[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │ not_equal_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_7     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │    \u001b[38;5;34m164,352\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │ not_equal_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │        \u001b[38;5;34m512\u001b[0m │ bidirectional_7[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │ not_equal_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │ not_equal_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_3       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ global_max_pooli… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │     \u001b[38;5;34m65,792\u001b[0m │ concatenate_3[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │      \u001b[38;5;34m1,024\u001b[0m │ dense_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m32,896\u001b[0m │ dropout_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m512\u001b[0m │ dense_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)         │        \u001b[38;5;34m645\u001b[0m │ dropout_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,603,653</span> (13.75 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,603,653\u001b[0m (13.75 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,602,117</span> (13.74 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,602,117\u001b[0m (13.74 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> (6.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,536\u001b[0m (6.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Starting training...\n",
      "Epoch 1/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3311 - loss: 3.5224\n",
      "Epoch 1: val_accuracy improved from -inf to 0.36351, saving model to arabic_nlp_optimized.keras\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m250s\u001b[0m 1s/step - accuracy: 0.3316 - loss: 3.5188 - val_accuracy: 0.3635 - val_loss: 3.5329 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.6451 - loss: 1.9983\n",
      "Epoch 2: val_accuracy improved from 0.36351 to 0.46154, saving model to arabic_nlp_optimized.keras\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m228s\u001b[0m 1s/step - accuracy: 0.6453 - loss: 1.9975 - val_accuracy: 0.4615 - val_loss: 2.3464 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7633 - loss: 1.4914\n",
      "Epoch 3: val_accuracy improved from 0.46154 to 0.71069, saving model to arabic_nlp_optimized.keras\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 1s/step - accuracy: 0.7634 - loss: 1.4909 - val_accuracy: 0.7107 - val_loss: 1.5978 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8179 - loss: 1.2147\n",
      "Epoch 4: val_accuracy improved from 0.71069 to 0.72192, saving model to arabic_nlp_optimized.keras\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 1s/step - accuracy: 0.8180 - loss: 1.2145 - val_accuracy: 0.7219 - val_loss: 1.5572 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8525 - loss: 1.0342\n",
      "Epoch 5: val_accuracy improved from 0.72192 to 0.73587, saving model to arabic_nlp_optimized.keras\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m224s\u001b[0m 1s/step - accuracy: 0.8525 - loss: 1.0341 - val_accuracy: 0.7359 - val_loss: 1.4570 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8695 - loss: 0.9367\n",
      "Epoch 6: val_accuracy did not improve from 0.73587\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 1s/step - accuracy: 0.8696 - loss: 0.9367 - val_accuracy: 0.7035 - val_loss: 1.6791 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8837 - loss: 0.8717\n",
      "Epoch 7: val_accuracy did not improve from 0.73587\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 1s/step - accuracy: 0.8838 - loss: 0.8716 - val_accuracy: 0.7304 - val_loss: 1.4983 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8922 - loss: 0.8265\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 8: val_accuracy did not improve from 0.73587\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m228s\u001b[0m 1s/step - accuracy: 0.8923 - loss: 0.8264 - val_accuracy: 0.7301 - val_loss: 1.5311 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9108 - loss: 0.7198\n",
      "Epoch 9: val_accuracy did not improve from 0.73587\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m229s\u001b[0m 1s/step - accuracy: 0.9109 - loss: 0.7194 - val_accuracy: 0.7314 - val_loss: 1.4967 - learning_rate: 5.0000e-04\n",
      "Epoch 10/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9425 - loss: 0.5689\n",
      "Epoch 10: val_accuracy improved from 0.73587 to 0.73656, saving model to arabic_nlp_optimized.keras\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m228s\u001b[0m 1s/step - accuracy: 0.9425 - loss: 0.5687 - val_accuracy: 0.7366 - val_loss: 1.4170 - learning_rate: 5.0000e-04\n",
      "Epoch 11/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9535 - loss: 0.4774\n",
      "Epoch 11: val_accuracy did not improve from 0.73656\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m231s\u001b[0m 1s/step - accuracy: 0.9535 - loss: 0.4773 - val_accuracy: 0.7284 - val_loss: 1.4851 - learning_rate: 5.0000e-04\n",
      "Epoch 12/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9527 - loss: 0.4457\n",
      "Epoch 12: val_accuracy did not improve from 0.73656\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m229s\u001b[0m 1s/step - accuracy: 0.9527 - loss: 0.4457 - val_accuracy: 0.7148 - val_loss: 1.5960 - learning_rate: 5.0000e-04\n",
      "Epoch 13/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9542 - loss: 0.4267\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 13: val_accuracy did not improve from 0.73656\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m228s\u001b[0m 1s/step - accuracy: 0.9542 - loss: 0.4266 - val_accuracy: 0.7263 - val_loss: 1.4604 - learning_rate: 5.0000e-04\n",
      "Epoch 14/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9637 - loss: 0.3832\n",
      "Epoch 14: val_accuracy did not improve from 0.73656\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 1s/step - accuracy: 0.9638 - loss: 0.3831 - val_accuracy: 0.7291 - val_loss: 1.5182 - learning_rate: 2.5000e-04\n",
      "Epoch 15/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9722 - loss: 0.3299\n",
      "Epoch 15: val_accuracy improved from 0.73656 to 0.74370, saving model to arabic_nlp_optimized.keras\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m228s\u001b[0m 1s/step - accuracy: 0.9723 - loss: 0.3298 - val_accuracy: 0.7437 - val_loss: 1.4600 - learning_rate: 2.5000e-04\n",
      "Epoch 16/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9736 - loss: 0.2933\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 16: val_accuracy did not improve from 0.74370\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 1s/step - accuracy: 0.9736 - loss: 0.2932 - val_accuracy: 0.7328 - val_loss: 1.4678 - learning_rate: 2.5000e-04\n",
      "Epoch 17/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9784 - loss: 0.2667\n",
      "Epoch 17: val_accuracy did not improve from 0.74370\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 1s/step - accuracy: 0.9784 - loss: 0.2666 - val_accuracy: 0.7437 - val_loss: 1.4658 - learning_rate: 1.2500e-04\n",
      "Epoch 18/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9831 - loss: 0.2408\n",
      "Epoch 18: val_accuracy did not improve from 0.74370\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m229s\u001b[0m 1s/step - accuracy: 0.9831 - loss: 0.2407 - val_accuracy: 0.7389 - val_loss: 1.5049 - learning_rate: 1.2500e-04\n",
      "Epoch 19/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9857 - loss: 0.2217\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 19: val_accuracy did not improve from 0.74370\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m228s\u001b[0m 1s/step - accuracy: 0.9857 - loss: 0.2216 - val_accuracy: 0.7386 - val_loss: 1.5241 - learning_rate: 1.2500e-04\n",
      "Epoch 20/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9879 - loss: 0.2030\n",
      "Epoch 20: val_accuracy did not improve from 0.74370\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m229s\u001b[0m 1s/step - accuracy: 0.9879 - loss: 0.2030 - val_accuracy: 0.7345 - val_loss: 1.5465 - learning_rate: 6.2500e-05\n",
      "Epoch 21/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9879 - loss: 0.1942\n",
      "Epoch 21: val_accuracy did not improve from 0.74370\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m229s\u001b[0m 1s/step - accuracy: 0.9880 - loss: 0.1942 - val_accuracy: 0.7355 - val_loss: 1.5527 - learning_rate: 6.2500e-05\n",
      "Epoch 22/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9899 - loss: 0.1847\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 22: val_accuracy did not improve from 0.74370\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m229s\u001b[0m 1s/step - accuracy: 0.9900 - loss: 0.1847 - val_accuracy: 0.7355 - val_loss: 1.5620 - learning_rate: 6.2500e-05\n",
      "Epoch 22: early stopping\n",
      "Restoring model weights from the end of the best epoch: 15.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.layers import (\n",
    "    Embedding, LSTM, Dense, Dropout, BatchNormalization, \n",
    "    Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D,\n",
    "    Concatenate, SpatialDropout1D\n",
    ")\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('stopwords', quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# GPU optimization\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "arabic_stopwords = set(stopwords.words('arabic'))\n",
    "\n",
    "# ==================== \n",
    "# 1. Enhanced Data Loading and Validation\n",
    "# ==================== \n",
    "def load_and_validate_data(file_path):\n",
    "    \"\"\"Load data with comprehensive validation\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, encoding='utf-8')\n",
    "        print(f\"Dataset loaded successfully with shape: {df.shape}\")\n",
    "        \n",
    "        # Check required columns\n",
    "        required_columns = ['text', 'new_label']  # Adjust based on your CSV structure\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            print(f\"Warning: Missing columns {missing_columns}\")\n",
    "            print(f\"Available columns: {df.columns.tolist()}\")\n",
    "        \n",
    "        # Data quality checks\n",
    "        print(f\"Missing values: {df.isnull().sum().sum()}\")\n",
    "        print(f\"Duplicate rows: {df.duplicated().sum()}\")\n",
    "        \n",
    "        # Remove duplicates and missing values\n",
    "        df = df.drop_duplicates()\n",
    "        df = df.dropna()\n",
    "        \n",
    "        # Class distribution\n",
    "        print(f\"Class distribution:\\n{df['new_label'].value_counts()}\")\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return None\n",
    "\n",
    "# ==================== \n",
    "# 2. Enhanced Arabic Text Preprocessing\n",
    "# ==================== \n",
    "def advanced_preprocess_arabic(text):\n",
    "    \"\"\"Enhanced Arabic text preprocessing\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    \n",
    "    # Remove emails\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Remove diacritics (tashkeel) - more comprehensive\n",
    "    text = re.sub(r'[\\u0617-\\u061A\\u064B-\\u0652\\u0670\\u0640]', '', text)\n",
    "    \n",
    "    # Remove tatweel (kashida)\n",
    "    text = re.sub(r'\\u0640+', '', text)\n",
    "    \n",
    "    # Remove punctuations, numbers, and special characters\n",
    "    text = re.sub(r'[^\\u0600-\\u06FF\\u0750-\\u077F\\u08A0-\\u08FF\\uFB50-\\uFDFF\\uFE70-\\uFEFF\\s]', ' ', text)\n",
    "    \n",
    "    # Normalize Arabic letters\n",
    "    text = re.sub(r'[إأآا]', 'ا', text)\n",
    "    text = re.sub(r'ى', 'ي', text)\n",
    "    text = re.sub(r'ؤ', 'و', text)\n",
    "    text = re.sub(r'ئ', 'ي', text)\n",
    "    text = re.sub(r'ة', 'ه', text)\n",
    "    text = re.sub(r'گ', 'ك', text)\n",
    "    \n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Remove stopwords and short words\n",
    "    words = text.split()\n",
    "    words = [w for w in words if w not in arabic_stopwords and len(w) > 2]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "# ==================== \n",
    "# 3. Data Analysis and Statistics\n",
    "# ==================== \n",
    "def analyze_data(df):\n",
    "    \"\"\"Analyze dataset statistics\"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"DATASET ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Text length analysis\n",
    "    df['text_length'] = df['text'].str.len()\n",
    "    df['word_count'] = df['text'].str.split().str.len()\n",
    "    \n",
    "    print(f\"Average text length: {df['text_length'].mean():.2f}\")\n",
    "    print(f\"Average word count: {df['word_count'].mean():.2f}\")\n",
    "    print(f\"Max text length: {df['text_length'].max()}\")\n",
    "    print(f\"Min text length: {df['text_length'].min()}\")\n",
    "    \n",
    "    # Class balance\n",
    "    class_counts = df['new_label'].value_counts()\n",
    "    print(f\"\\nClass distribution:\")\n",
    "    for label, count in class_counts.items():\n",
    "        print(f\"{label}: {count} ({count/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ==================== \n",
    "# 4. Advanced Model Architecture\n",
    "# ==================== \n",
    "def create_advanced_model(vocab_size, max_len, num_classes, embedding_dim=200):\n",
    "    \"\"\"Create an advanced model with multiple techniques\"\"\"\n",
    "    \n",
    "    # Input layer\n",
    "    input_layer = tf.keras.layers.Input(shape=(max_len,))\n",
    "    \n",
    "    # Embedding layer with dropout\n",
    "    embedding = Embedding(\n",
    "        vocab_size, \n",
    "        embedding_dim, \n",
    "        input_length=max_len,\n",
    "        mask_zero=True,\n",
    "        embeddings_regularizer=l2(0.001)\n",
    "    )(input_layer)\n",
    "    \n",
    "    # Spatial dropout for embedding\n",
    "    embedding = SpatialDropout1D(0.2)(embedding)\n",
    "    \n",
    "    # Bidirectional LSTM layers\n",
    "    lstm1 = Bidirectional(LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.3))(embedding)\n",
    "    lstm1 = BatchNormalization()(lstm1)\n",
    "    \n",
    "    lstm2 = Bidirectional(LSTM(64, return_sequences=True, dropout=0.3, recurrent_dropout=0.3))(lstm1)\n",
    "    lstm2 = BatchNormalization()(lstm2)\n",
    "    \n",
    "    # Global pooling layers\n",
    "    max_pool = GlobalMaxPooling1D()(lstm2)\n",
    "    avg_pool = GlobalAveragePooling1D()(lstm2)\n",
    "    \n",
    "    # Concatenate pooling outputs\n",
    "    concat = Concatenate()([max_pool, avg_pool])\n",
    "    \n",
    "    # Dense layers with regularization\n",
    "    dense1 = Dense(256, activation='relu', kernel_regularizer=l2(0.001))(concat)\n",
    "    dense1 = BatchNormalization()(dense1)\n",
    "    dense1 = Dropout(0.5)(dense1)\n",
    "    \n",
    "    dense2 = Dense(128, activation='relu', kernel_regularizer=l2(0.001))(dense1)\n",
    "    dense2 = BatchNormalization()(dense2)\n",
    "    dense2 = Dropout(0.3)(dense2)\n",
    "    \n",
    "    # Output layer\n",
    "    if num_classes == 2:\n",
    "        output = Dense(1, activation='sigmoid', name='output')(dense2)\n",
    "        loss = 'binary_crossentropy'\n",
    "    else:\n",
    "        output = Dense(num_classes, activation='softmax', name='output')(dense2)\n",
    "        loss = 'sparse_categorical_crossentropy'\n",
    "    \n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=output)\n",
    "    \n",
    "    # Advanced optimizer with learning rate scheduling\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=0.001,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-07,\n",
    "        clipnorm=1.0\n",
    "    )\n",
    "    \n",
    "    if num_classes == 2:\n",
    "        metrics = ['accuracy', 'precision', 'recall']\n",
    "    else:\n",
    "        metrics = ['accuracy']\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss,\n",
    "        metrics=metrics\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# ==================== \n",
    "# 5. Advanced Training Configuration\n",
    "# ==================== \n",
    "def create_callbacks(model_name=\"best_model.keras\"):\n",
    "    \"\"\"Create advanced callbacks for training\"\"\"\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=7,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1,\n",
    "            mode='max'\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=3,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1,\n",
    "            mode='min'\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            model_name,\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            save_weights_only=False,\n",
    "            mode='max',\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    return callbacks\n",
    "\n",
    "# ==================== \n",
    "# 6. Main Training Pipeline\n",
    "# ==================== \n",
    "# Load and validate data\n",
    "df = load_and_validate_data(\"Arabic_dataset.csv\")\n",
    "\n",
    "# Analyze data\n",
    "df = analyze_data(df)\n",
    "    \n",
    "# Preprocess text data\n",
    "print(\"Preprocessing text data...\")\n",
    "df[\"text\"] = df[\"text\"].astype(str).apply(advanced_preprocess_arabic)\n",
    "    \n",
    "# Remove empty texts after preprocessing\n",
    "df = df[df['text'].str.len() > 0]\n",
    "    \n",
    "texts = df[\"text\"].tolist()\n",
    "labels = df[\"new_label\"].tolist()\n",
    "    \n",
    "# ==================== \n",
    "# Tokenization with optimization\n",
    "# ==================== \n",
    "vocab_size = 15000  # Increased vocabulary\n",
    "max_len = 150       # Optimized sequence length\n",
    "    \n",
    "tokenizer = Tokenizer(\n",
    "        num_words=vocab_size, \n",
    "        oov_token=\"<OOV>\",\n",
    "        filters='',  # We already preprocessed\n",
    "        lower=False  # Arabic doesn't have upper/lower case\n",
    "    )\n",
    "    \n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "padded = pad_sequences(sequences, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
    "    \n",
    "# Save tokenizer\n",
    "with open(\"tokenizer_optimized.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "print(f\"Vocabulary size: {len(tokenizer.word_index)}\")\n",
    "print(f\"Average sequence length: {np.mean([len(seq) for seq in sequences]):.2f}\")\n",
    "    \n",
    "# ==================== \n",
    "# Label encoding\n",
    "# ==================== \n",
    "encoder = LabelEncoder()\n",
    "encoded_labels = encoder.fit_transform(labels)\n",
    "num_classes = len(encoder.classes_)\n",
    "    \n",
    "# Save label encoder\n",
    "with open(\"label_encoder_optimized.pickle\", \"wb\") as f:\n",
    "    pickle.dump(encoder, f)\n",
    "    \n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Classes: {encoder.classes_}\")\n",
    "    \n",
    "# ==================== \n",
    "# Advanced data split with stratification\n",
    "# ==================== \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        padded, encoded_labels, \n",
    "        test_size=0.2, \n",
    "        random_state=42,\n",
    "        stratify=encoded_labels\n",
    "    )\n",
    "    \n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train, y_train,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=y_train\n",
    "    )\n",
    "    \n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "    \n",
    "# ==================== \n",
    "# Handle class imbalance\n",
    "# ==================== \n",
    "class_weights = compute_class_weight(\n",
    "        'balanced',\n",
    "        classes=np.unique(y_train),\n",
    "        y=y_train\n",
    "    )\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "print(f\"Class weights: {class_weight_dict}\")\n",
    "    \n",
    "# ==================== \n",
    "# Create and train model\n",
    "# ==================== \n",
    "model = create_advanced_model(vocab_size, max_len, num_classes)\n",
    "print(model.summary())\n",
    "    \n",
    "# Training configuration\n",
    "callbacks = create_callbacks(\"arabic_nlp_optimized.keras\")\n",
    "    \n",
    "# Train model\n",
    "print(\"Starting training...\")\n",
    "history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=100,\n",
    "        batch_size=64,  # Optimized batch size\n",
    "        callbacks=callbacks,\n",
    "        class_weight=class_weight_dict,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab7cc50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model...\n",
      "Test Accuracy: 1.4772\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 155ms/step\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Anxiety       0.73      0.77      0.75       832\n",
      "  Depression       0.76      0.78      0.77       850\n",
      "      Normal       0.77      0.73      0.75      1335\n",
      "      Stress       0.57      0.56      0.57       536\n",
      "     Suicide       0.74      0.79      0.76       120\n",
      "\n",
      "    accuracy                           0.73      3673\n",
      "   macro avg       0.71      0.73      0.72      3673\n",
      "weighted avg       0.73      0.73      0.73      3673\n",
      "\n",
      "\n",
      "Training completed successfully!\n",
      "Saved files:\n",
      "- arabic_nlp_optimized.keras (model)\n",
      "- tokenizer_optimized.pickle\n",
      "- label_encoder_optimized.pickle\n",
      "- training_history.pickle\n"
     ]
    }
   ],
   "source": [
    "# ==================== \n",
    "# Evaluation\n",
    "# ==================== \n",
    "print(\"\\nEvaluating model...\")\n",
    "    \n",
    "# Load best model\n",
    "best_model = tf.keras.models.load_model(\n",
    "        \"arabic_nlp_optimized.keras\"\n",
    "    )\n",
    "    \n",
    "# Evaluate on test set\n",
    "eval_results = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "if num_classes == 2:\n",
    "        test_accuracy, test_precision, test_recall = eval_results\n",
    "        f1_score = 2 * (test_precision * test_recall) / (test_precision + test_recall)\n",
    "        print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "        print(f\"Test Precision: {test_precision:.4f}\")\n",
    "        print(f\"Test Recall: {test_recall:.4f}\")\n",
    "        print(f\"Test F1-Score: {f1_score:.4f}\")\n",
    "else:\n",
    "        test_accuracy = eval_results[0]\n",
    "        print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "# Predictions and classification report\n",
    "y_pred = best_model.predict(X_test)\n",
    "if num_classes == 2:\n",
    "        y_pred_classes = (y_pred > 0.5).astype(int).flatten()\n",
    "else:\n",
    "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    \n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_classes, target_names=encoder.classes_))\n",
    " \n",
    "# ==================== \n",
    "# Save training history\n",
    "# ==================== \n",
    "with open(\"training_history.pickle\", \"wb\") as f:\n",
    "        pickle.dump(history.history, f)\n",
    "    \n",
    "print(\"\\nTraining completed successfully!\")\n",
    "print(\"Saved files:\")\n",
    "print(\"- arabic_nlp_optimized.keras (model)\")\n",
    "print(\"- tokenizer_optimized.pickle\")\n",
    "print(\"- label_encoder_optimized.pickle\")\n",
    "print(\"- training_history.pickle\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
